{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sequitur.models import LINEAR_AE, LSTM_AE\n",
    "from sequitur import quick_train\n",
    "from sequitur.quick_train import train_model\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from finta import TA\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_seqs = [torch.randn(4) for _ in range(100)] # 100 sequences of length 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder, _, _ = quick_train(LINEAR_AE, train_seqs, encoding_dim=2, denoise=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.9200, -0.3487], grad_fn=<TanhBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.randn(4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.8829, -0.1512],\n        [ 0.2971,  0.7620]], grad_fn=<TanhBackward0>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.randn(2, 4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sequence of 1D vectors (perfect for our purposes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = LSTM_AE(\n",
    "  input_dim=3,\n",
    "  encoding_dim=7,\n",
    "  h_dims=[64],\n",
    "  h_activ=None,\n",
    "  out_activ=None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "x = torch.randn(10, 3) # Sequence of 10 3D vectors\n",
    "z = model.encoder(x) # z.shape = [7]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "x_prime = model.decoder(z, seq_len=10) # x_prime.shape = [10, 3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1040, 0.1869, 0.1644],\n        [0.1417, 0.2564, 0.2296],\n        [0.1515, 0.2775, 0.2519],\n        [0.1514, 0.2804, 0.2570],\n        [0.1486, 0.2773, 0.2558],\n        [0.1459, 0.2731, 0.2530],\n        [0.1438, 0.2693, 0.2503],\n        [0.1424, 0.2664, 0.2481],\n        [0.1415, 0.2642, 0.2466],\n        [0.1409, 0.2627, 0.2455]], grad_fn=<MmBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_prime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trying it out on real data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def create_ta_features(ticker='^GSPC', start_='2010-01-01', end_='2022-12-31', interval_='1d', fillna=True, scale_to_std=True, fill_weekends=True):\n",
    "    \"\"\"\n",
    "    Creates dataframe with technical analysis features\n",
    "    :param ticker: ticker symbol to download data for (default is S&P 500)\n",
    "    :param start_: start date\n",
    "    :param end_: end date\n",
    "    :param interval_: data frequency\n",
    "    :param fillna: whether to fill in missing values\n",
    "    :param scale_to_std: whether to scale to standard deviation\n",
    "    :param fill_weekends: whether to fill in weekends\n",
    "    :return: dataframe with technical analysis features\n",
    "    \"\"\"\n",
    "    # download data\n",
    "    df = yf.download(ticker, start_, end_, interval=interval_)\n",
    "    # rename columns\n",
    "    df.rename(columns={\"Open\": \"open\", \"Adj Close\": \"close\", \"High\": \"high\", \"Low\": \"low\", \"Volume\": \"volume\"}, inplace=True)\n",
    "    # drop close column\n",
    "    df.drop(\"Close\", inplace=True, axis=1)\n",
    "    # fill weekends\n",
    "    if fill_weekends:\n",
    "        df = df.resample('D').ffill()\n",
    "    # get all functions in finta\n",
    "    finta_functions = [func for func in dir(TA) if callable(getattr(TA, func)) and not func.startswith(\"__\")]\n",
    "    # loop through all functions in finta and append the results to the dataframe\n",
    "    # skip functions that throw errors\n",
    "    for func in finta_functions:\n",
    "        try:\n",
    "            df[func] = getattr(TA, func)(df)\n",
    "        except:\n",
    "            pass\n",
    "    # fill in missing values\n",
    "    if fillna:\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "    # scale to standard deviation, by column\n",
    "    if scale_to_std:\n",
    "        df = (df - df.mean()) / df.std()\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# function that adds sine and cosine of weekday, monthday, yearday to dataframe\n",
    "# takes into account whether the data is daily, hourly, minutely, etc.\n",
    "# also takes into account whether data includes weekends or not\n",
    "# if data does not include weekends, assume the week is 5 days, not 7, month is 21 days, not 31, and year is 250 days, not 365\n",
    "def add_time_features(df):\n",
    "    \"\"\"\n",
    "    Adds sine and cosine of weekday, monthday, yearday to dataframe\n",
    "    :param df: dataframe to add time features to\n",
    "    :return: dataframe with time features\n",
    "    \"\"\"\n",
    "    # get frequency of data\n",
    "    freq = pd.infer_freq(df.index)\n",
    "    # if frequency is daily, assume data includes weekends\n",
    "    if freq == 'D':\n",
    "        include_weekends = True\n",
    "    else:\n",
    "        include_weekends = False\n",
    "\n",
    "    # get number of days in week, month, year\n",
    "    if include_weekends:\n",
    "        days_in_week = 7\n",
    "        days_in_month = 31\n",
    "        days_in_year = 365\n",
    "    else:\n",
    "        days_in_week = 5\n",
    "        days_in_month = 21\n",
    "        days_in_year = 250\n",
    "    # add weekday, monthday, yearday features\n",
    "    df['weekday'] = df.index.dayofweek\n",
    "    df['monthday'] = df.index.day\n",
    "    df['yearday'] = df.index.dayofyear\n",
    "    # add sine and cosine of weekday, monthday, yearday features\n",
    "    df['sin_weekday'] = np.sin(2 * np.pi * df['weekday'] / days_in_week)\n",
    "    df['cos_weekday'] = np.cos(2 * np.pi * df['weekday'] / days_in_week)\n",
    "    df['sin_monthday'] = np.sin(2 * np.pi * df['monthday'] / days_in_month)\n",
    "    df['cos_monthday'] = np.cos(2 * np.pi * df['monthday'] / days_in_month)\n",
    "    df['sin_yearday'] = np.sin(2 * np.pi * df['yearday'] / days_in_year)\n",
    "    df['cos_yearday'] = np.cos(2 * np.pi * df['yearday'] / days_in_year)\n",
    "    # drop weekday, monthday, yearday features\n",
    "    df.drop(['weekday', 'monthday', 'yearday'], inplace=True, axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def sliding_window(df, window_size=10):\n",
    "    \"\"\"\n",
    "    Creates a sliding window mechanism for a given dataframe\n",
    "    :param df: dataframe\n",
    "    :param window_size: window size\n",
    "    :return: list of windows as dataframes\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for i in range(len(df) - window_size + 1):\n",
    "        windows.append(df.iloc[i:i + window_size])\n",
    "    return windows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\finta\\finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n",
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\finta\\finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n",
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\finta\\finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n",
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\finta\\finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n",
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\finta\\finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n",
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\finta\\finta.py:292: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  sc.iteritems(), sma.shift().iteritems(), ohlc[column].iteritems()\n"
     ]
    },
    {
     "data": {
      "text/plain": "                open      high       low     close    volume       ADL  \\\nDate                                                                     \n2010-01-04 -1.299113 -1.286708 -1.293664 -1.282928  0.049746 -1.833595   \n2010-01-05 -1.282826 -1.283933 -1.280318 -1.279357 -1.377868 -1.830133   \n2010-01-06 -1.279741 -1.281359 -1.275947 -1.278730  0.983417 -1.828580   \n2010-01-07 -1.279174 -1.278071 -1.278627 -1.274127  1.266984 -1.822062   \n2010-01-08 -1.274875 -1.275126 -1.273635 -1.270798  0.428625 -1.816325   \n2010-01-09 -1.274875 -1.275126 -1.273635 -1.270798  0.428625 -1.810589   \n2010-01-10 -1.274875 -1.275126 -1.273635 -1.270798  0.428625 -1.804853   \n2010-01-11 -1.269371 -1.270752 -1.267726 -1.268775  0.301304 -1.803112   \n2010-01-12 -1.271546 -1.276714 -1.278168 -1.279660  0.739357 -1.804878   \n2010-01-13 -1.278122 -1.272099 -1.276732 -1.270090  0.220027 -1.801032   \n2010-01-14 -1.269655 -1.270079 -1.265912 -1.267278 -0.022758 -1.798728   \n2010-01-15 -1.267591 -1.272733 -1.278555 -1.279853  0.779862 -1.801688   \n2010-01-16 -1.267591 -1.272733 -1.278555 -1.279853  0.779862 -1.804649   \n2010-01-17 -1.267591 -1.272733 -1.278555 -1.279853  0.779862 -1.807609   \n2010-01-18 -1.267591 -1.272733 -1.278555 -1.279853  0.779862 -1.810569   \n2010-01-19 -1.279417 -1.270038 -1.274093 -1.265487  0.747606 -1.803992   \n2010-01-20 -1.267358 -1.272552 -1.280735 -1.277819  0.829179 -1.804405   \n2010-01-21 -1.276736 -1.278956 -1.295416 -1.299631  2.792822 -1.813060   \n2010-01-22 -1.300195 -1.305186 -1.320539 -1.324639  2.159464 -1.820858   \n2010-01-23 -1.300195 -1.305186 -1.320539 -1.324639  2.159464 -1.828656   \n\n                 ADX        AO       ATR   BBWIDTH  ...      VAMA       VBM  \\\nDate                                                ...                       \n2010-01-04  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-05  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-06  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-07  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-08  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-09  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-10  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-11  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-12  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-13  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-14  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-15  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-16  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-17  6.432831 -0.703535 -0.730905  0.147278  ... -1.274752  0.604053   \n2010-01-18  6.432831 -0.703535 -0.733667  0.147278  ... -1.274752  0.604053   \n2010-01-19  6.432831 -0.703535 -0.710762  0.147278  ... -1.274298  0.604053   \n2010-01-20  3.714898 -0.703535 -0.664000  0.147278  ... -1.274047  0.604053   \n2010-01-21  3.951982 -0.703535 -0.617655  0.147278  ... -1.278467  0.604053   \n2010-01-22  4.360976 -0.703535 -0.566764  0.147278  ... -1.285501  0.604053   \n2010-01-23  4.605477 -0.703535 -0.518814  0.147278  ... -1.291048  0.604053   \n\n                 VFI       VPT      VWAP       VZO  WILLIAMS       WMA  \\\nDate                                                                     \n2010-01-04 -0.437061 -1.758947 -1.302675 -0.247649 -0.179769 -1.272791   \n2010-01-05 -0.437061 -1.755258 -1.296173  1.659163 -0.179769 -1.272791   \n2010-01-06 -0.437061 -1.751630 -1.288821  2.959055 -0.179769 -1.272791   \n2010-01-07 -0.437061 -1.744774 -1.284374  3.464771 -0.179769 -1.272791   \n2010-01-08 -0.437061 -1.739065 -1.280347  3.687970 -0.179769 -1.272791   \n2010-01-09 -0.437061 -1.733357 -1.277707  2.766435 -0.179769 -1.272791   \n2010-01-10 -0.437061 -1.727649 -1.275841  2.125317 -0.179769 -1.272791   \n2010-01-11 -0.437061 -1.726146 -1.273170  2.544684 -0.179769 -1.272791   \n2010-01-12 -0.437061 -1.734095 -1.273745  1.095195 -0.179769 -1.272791   \n2010-01-13 -0.437061 -1.727963 -1.272841  1.633398 -0.179769 -1.271787   \n2010-01-14 -0.437061 -1.723560 -1.271018  2.044156 -0.179769 -1.270505   \n2010-01-15 -0.437061 -1.732641 -1.271380  0.830514 -0.179769 -1.272009   \n2010-01-16 -0.437061 -1.741721 -1.271681  0.647438 -0.179769 -1.273487   \n2010-01-17 -0.437061 -1.750801 -1.271935  0.500797 -0.179769 -1.274838   \n2010-01-18 -0.437061 -1.759882 -1.272153  0.381809 -1.030983 -1.275988   \n2010-01-19 -0.437061 -1.747662 -1.271100  0.986582  1.132036 -1.274062   \n2010-01-20 -0.437061 -1.754478 -1.271343  0.099364 -0.689358 -1.274722   \n2010-01-21 -0.437061 -1.769737 -1.274723 -0.889447 -1.859144 -1.279545   \n2010-01-22 -0.437061 -1.785301 -1.281939 -1.569147 -1.922114 -1.288928   \n2010-01-23 -0.437061 -1.800865 -1.288210 -1.348504 -1.922114 -1.297099   \n\n                WOBV     ZLEMA  \nDate                            \n2010-01-04 -1.100389 -1.275609  \n2010-01-05 -1.100389 -1.275609  \n2010-01-06 -1.098638 -1.275609  \n2010-01-07 -1.085015 -1.275609  \n2010-01-08 -1.076811 -1.275609  \n2010-01-09 -1.076811 -1.275609  \n2010-01-10 -1.076811 -1.275609  \n2010-01-11 -1.071975 -1.275609  \n2010-01-12 -1.100803 -1.275609  \n2010-01-13 -1.078391 -1.275609  \n2010-01-14 -1.072208 -1.275609  \n2010-01-15 -1.105811 -1.275609  \n2010-01-16 -1.105811 -1.275609  \n2010-01-17 -1.105811 -1.277462  \n2010-01-18 -1.105811 -1.278304  \n2010-01-19 -1.067697 -1.271983  \n2010-01-20 -1.101009 -1.274692  \n2010-01-21 -1.185204 -1.285222  \n2010-01-22 -1.272392 -1.301588  \n2010-01-23 -1.272392 -1.314104  \n\n[20 rows x 66 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>ADL</th>\n      <th>ADX</th>\n      <th>AO</th>\n      <th>ATR</th>\n      <th>BBWIDTH</th>\n      <th>...</th>\n      <th>VAMA</th>\n      <th>VBM</th>\n      <th>VFI</th>\n      <th>VPT</th>\n      <th>VWAP</th>\n      <th>VZO</th>\n      <th>WILLIAMS</th>\n      <th>WMA</th>\n      <th>WOBV</th>\n      <th>ZLEMA</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-04</th>\n      <td>-1.299113</td>\n      <td>-1.286708</td>\n      <td>-1.293664</td>\n      <td>-1.282928</td>\n      <td>0.049746</td>\n      <td>-1.833595</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.758947</td>\n      <td>-1.302675</td>\n      <td>-0.247649</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.100389</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-05</th>\n      <td>-1.282826</td>\n      <td>-1.283933</td>\n      <td>-1.280318</td>\n      <td>-1.279357</td>\n      <td>-1.377868</td>\n      <td>-1.830133</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.755258</td>\n      <td>-1.296173</td>\n      <td>1.659163</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.100389</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-06</th>\n      <td>-1.279741</td>\n      <td>-1.281359</td>\n      <td>-1.275947</td>\n      <td>-1.278730</td>\n      <td>0.983417</td>\n      <td>-1.828580</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.751630</td>\n      <td>-1.288821</td>\n      <td>2.959055</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.098638</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-07</th>\n      <td>-1.279174</td>\n      <td>-1.278071</td>\n      <td>-1.278627</td>\n      <td>-1.274127</td>\n      <td>1.266984</td>\n      <td>-1.822062</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.744774</td>\n      <td>-1.284374</td>\n      <td>3.464771</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.085015</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-08</th>\n      <td>-1.274875</td>\n      <td>-1.275126</td>\n      <td>-1.273635</td>\n      <td>-1.270798</td>\n      <td>0.428625</td>\n      <td>-1.816325</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.739065</td>\n      <td>-1.280347</td>\n      <td>3.687970</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.076811</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-09</th>\n      <td>-1.274875</td>\n      <td>-1.275126</td>\n      <td>-1.273635</td>\n      <td>-1.270798</td>\n      <td>0.428625</td>\n      <td>-1.810589</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.733357</td>\n      <td>-1.277707</td>\n      <td>2.766435</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.076811</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-10</th>\n      <td>-1.274875</td>\n      <td>-1.275126</td>\n      <td>-1.273635</td>\n      <td>-1.270798</td>\n      <td>0.428625</td>\n      <td>-1.804853</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.727649</td>\n      <td>-1.275841</td>\n      <td>2.125317</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.076811</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-11</th>\n      <td>-1.269371</td>\n      <td>-1.270752</td>\n      <td>-1.267726</td>\n      <td>-1.268775</td>\n      <td>0.301304</td>\n      <td>-1.803112</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.726146</td>\n      <td>-1.273170</td>\n      <td>2.544684</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.071975</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-12</th>\n      <td>-1.271546</td>\n      <td>-1.276714</td>\n      <td>-1.278168</td>\n      <td>-1.279660</td>\n      <td>0.739357</td>\n      <td>-1.804878</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.734095</td>\n      <td>-1.273745</td>\n      <td>1.095195</td>\n      <td>-0.179769</td>\n      <td>-1.272791</td>\n      <td>-1.100803</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-13</th>\n      <td>-1.278122</td>\n      <td>-1.272099</td>\n      <td>-1.276732</td>\n      <td>-1.270090</td>\n      <td>0.220027</td>\n      <td>-1.801032</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.727963</td>\n      <td>-1.272841</td>\n      <td>1.633398</td>\n      <td>-0.179769</td>\n      <td>-1.271787</td>\n      <td>-1.078391</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-14</th>\n      <td>-1.269655</td>\n      <td>-1.270079</td>\n      <td>-1.265912</td>\n      <td>-1.267278</td>\n      <td>-0.022758</td>\n      <td>-1.798728</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.723560</td>\n      <td>-1.271018</td>\n      <td>2.044156</td>\n      <td>-0.179769</td>\n      <td>-1.270505</td>\n      <td>-1.072208</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-15</th>\n      <td>-1.267591</td>\n      <td>-1.272733</td>\n      <td>-1.278555</td>\n      <td>-1.279853</td>\n      <td>0.779862</td>\n      <td>-1.801688</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.732641</td>\n      <td>-1.271380</td>\n      <td>0.830514</td>\n      <td>-0.179769</td>\n      <td>-1.272009</td>\n      <td>-1.105811</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-16</th>\n      <td>-1.267591</td>\n      <td>-1.272733</td>\n      <td>-1.278555</td>\n      <td>-1.279853</td>\n      <td>0.779862</td>\n      <td>-1.804649</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.741721</td>\n      <td>-1.271681</td>\n      <td>0.647438</td>\n      <td>-0.179769</td>\n      <td>-1.273487</td>\n      <td>-1.105811</td>\n      <td>-1.275609</td>\n    </tr>\n    <tr>\n      <th>2010-01-17</th>\n      <td>-1.267591</td>\n      <td>-1.272733</td>\n      <td>-1.278555</td>\n      <td>-1.279853</td>\n      <td>0.779862</td>\n      <td>-1.807609</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.730905</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.750801</td>\n      <td>-1.271935</td>\n      <td>0.500797</td>\n      <td>-0.179769</td>\n      <td>-1.274838</td>\n      <td>-1.105811</td>\n      <td>-1.277462</td>\n    </tr>\n    <tr>\n      <th>2010-01-18</th>\n      <td>-1.267591</td>\n      <td>-1.272733</td>\n      <td>-1.278555</td>\n      <td>-1.279853</td>\n      <td>0.779862</td>\n      <td>-1.810569</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.733667</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274752</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.759882</td>\n      <td>-1.272153</td>\n      <td>0.381809</td>\n      <td>-1.030983</td>\n      <td>-1.275988</td>\n      <td>-1.105811</td>\n      <td>-1.278304</td>\n    </tr>\n    <tr>\n      <th>2010-01-19</th>\n      <td>-1.279417</td>\n      <td>-1.270038</td>\n      <td>-1.274093</td>\n      <td>-1.265487</td>\n      <td>0.747606</td>\n      <td>-1.803992</td>\n      <td>6.432831</td>\n      <td>-0.703535</td>\n      <td>-0.710762</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274298</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.747662</td>\n      <td>-1.271100</td>\n      <td>0.986582</td>\n      <td>1.132036</td>\n      <td>-1.274062</td>\n      <td>-1.067697</td>\n      <td>-1.271983</td>\n    </tr>\n    <tr>\n      <th>2010-01-20</th>\n      <td>-1.267358</td>\n      <td>-1.272552</td>\n      <td>-1.280735</td>\n      <td>-1.277819</td>\n      <td>0.829179</td>\n      <td>-1.804405</td>\n      <td>3.714898</td>\n      <td>-0.703535</td>\n      <td>-0.664000</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.274047</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.754478</td>\n      <td>-1.271343</td>\n      <td>0.099364</td>\n      <td>-0.689358</td>\n      <td>-1.274722</td>\n      <td>-1.101009</td>\n      <td>-1.274692</td>\n    </tr>\n    <tr>\n      <th>2010-01-21</th>\n      <td>-1.276736</td>\n      <td>-1.278956</td>\n      <td>-1.295416</td>\n      <td>-1.299631</td>\n      <td>2.792822</td>\n      <td>-1.813060</td>\n      <td>3.951982</td>\n      <td>-0.703535</td>\n      <td>-0.617655</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.278467</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.769737</td>\n      <td>-1.274723</td>\n      <td>-0.889447</td>\n      <td>-1.859144</td>\n      <td>-1.279545</td>\n      <td>-1.185204</td>\n      <td>-1.285222</td>\n    </tr>\n    <tr>\n      <th>2010-01-22</th>\n      <td>-1.300195</td>\n      <td>-1.305186</td>\n      <td>-1.320539</td>\n      <td>-1.324639</td>\n      <td>2.159464</td>\n      <td>-1.820858</td>\n      <td>4.360976</td>\n      <td>-0.703535</td>\n      <td>-0.566764</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.285501</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.785301</td>\n      <td>-1.281939</td>\n      <td>-1.569147</td>\n      <td>-1.922114</td>\n      <td>-1.288928</td>\n      <td>-1.272392</td>\n      <td>-1.301588</td>\n    </tr>\n    <tr>\n      <th>2010-01-23</th>\n      <td>-1.300195</td>\n      <td>-1.305186</td>\n      <td>-1.320539</td>\n      <td>-1.324639</td>\n      <td>2.159464</td>\n      <td>-1.828656</td>\n      <td>4.605477</td>\n      <td>-0.703535</td>\n      <td>-0.518814</td>\n      <td>0.147278</td>\n      <td>...</td>\n      <td>-1.291048</td>\n      <td>0.604053</td>\n      <td>-0.437061</td>\n      <td>-1.800865</td>\n      <td>-1.288210</td>\n      <td>-1.348504</td>\n      <td>-1.922114</td>\n      <td>-1.297099</td>\n      <td>-1.272392</td>\n      <td>-1.314104</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows Ã— 66 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500_df = create_ta_features()\n",
    "sp500_df.head(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "financial_data_autoencoder = LSTM_AE(\n",
    "  input_dim=sp500_df.shape[1],\n",
    "  encoding_dim=32,\n",
    "  h_dims=[128, 64],\n",
    "  h_activ=None,\n",
    "  out_activ=None\n",
    ")\n",
    "# move model to GPU\n",
    "financial_data_autoencoder = financial_data_autoencoder.to('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "training_data = torch.tensor(sp500_df.values, dtype=torch.float32, device='cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([66])) that is different to the input size (torch.Size([66, 66])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1693.452533261973\n",
      "Epoch: 2, Loss: 1280.6485426807885\n",
      "Epoch: 3, Loss: 1217.8714202829397\n",
      "Epoch: 4, Loss: 1224.7812482985034\n",
      "Epoch: 5, Loss: 1177.7293147686962\n",
      "Epoch: 6, Loss: 1166.7640051688954\n",
      "Epoch: 7, Loss: 1165.7868077244364\n",
      "Epoch: 8, Loss: 1168.6432410653485\n",
      "Epoch: 9, Loss: 1158.4890689142237\n",
      "Epoch: 10, Loss: 1154.796830008444\n",
      "Epoch: 11, Loss: 1147.761008187048\n",
      "Epoch: 12, Loss: 1152.2500398195211\n",
      "Epoch: 13, Loss: 1152.7655885030608\n",
      "Epoch: 14, Loss: 1132.8372334485111\n",
      "Epoch: 15, Loss: 1138.2689755360955\n",
      "Epoch: 16, Loss: 1154.133754000093\n",
      "Epoch: 17, Loss: 1144.8016871827047\n",
      "Epoch: 18, Loss: 1142.7148389092586\n",
      "Epoch: 19, Loss: 1156.1241617636979\n",
      "Epoch: 20, Loss: 1146.193232332072\n",
      "Epoch: 21, Loss: 1147.5817251285855\n",
      "Epoch: 22, Loss: 1137.3829590281186\n",
      "Epoch: 23, Loss: 1141.513501728566\n",
      "Epoch: 24, Loss: 1136.6895291423316\n",
      "Epoch: 25, Loss: 1137.9719301612945\n",
      "Epoch: 26, Loss: 1122.084078282168\n",
      "Epoch: 27, Loss: 1113.3859779911138\n",
      "Epoch: 28, Loss: 1127.7870183240302\n",
      "Epoch: 29, Loss: 1112.7092895765127\n",
      "Epoch: 30, Loss: 1124.2076822348429\n",
      "Epoch: 31, Loss: 1127.6918384783585\n",
      "Epoch: 32, Loss: 1125.0308154879694\n",
      "Epoch: 33, Loss: 1129.4524911298317\n",
      "Epoch: 34, Loss: 1138.7245498612197\n",
      "Epoch: 35, Loss: 1100.9052324053812\n",
      "Epoch: 36, Loss: 1121.8483655778393\n",
      "Epoch: 37, Loss: 1131.9686804798764\n",
      "Epoch: 38, Loss: 1133.0191294491592\n",
      "Epoch: 39, Loss: 1121.9263835861952\n",
      "Epoch: 40, Loss: 1137.6310706307474\n",
      "Epoch: 41, Loss: 1123.531064726651\n",
      "Epoch: 42, Loss: 1115.2448175697261\n",
      "Epoch: 43, Loss: 1100.7196052544837\n",
      "Epoch: 44, Loss: 1119.1472702605327\n",
      "Epoch: 45, Loss: 1101.9586896044007\n",
      "Epoch: 46, Loss: 1120.730357184579\n",
      "Epoch: 47, Loss: 1096.5712901142758\n",
      "Epoch: 48, Loss: 1105.156080818498\n",
      "Epoch: 49, Loss: 1096.673182524477\n",
      "Epoch: 50, Loss: 1087.0606953394151\n",
      "Epoch: 51, Loss: 1078.3030351088782\n",
      "Epoch: 52, Loss: 1329.129677597226\n",
      "Epoch: 53, Loss: 1263.3286324351473\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [35], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfinancial_data_autoencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenoise\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\sequitur\\quick_train.py:44\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_set, verbose, lr, epochs, denoise)\u001B[0m\n\u001B[0;32m     41\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m---> 44\u001B[0m x_prime \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(x_prime, x)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\sequitur\\models\\lstm_ae.py:92\u001B[0m, in \u001B[0;36mLSTM_AE.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     91\u001B[0m     seq_len \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 92\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     93\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(x, seq_len)\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\sequitur\\models\\lstm_ae.py:32\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     30\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers):\n\u001B[1;32m---> 32\u001B[0m     x, (h_n, c_n) \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh_activ \u001B[38;5;129;01mand\u001B[39;00m index \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     35\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh_activ(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:787\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    785\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    786\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched:\n\u001B[1;32m--> 787\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    788\u001B[0m         hidden \u001B[38;5;241m=\u001B[39m (hidden[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m), hidden[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    789\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hidden, unsorted_indices)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "losses = train_model(financial_data_autoencoder, training_data, epochs=100, verbose=True, lr=1e-3, denoise=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining non LSTM autoencoder just using linear layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# define encoder class which will take in a sliding window of stock data 2d tensor and transform it into a 1d tensor\n",
    "class LinearEncoder(nn.Module):\n",
    "    def __init__(self, input_dims: Tuple[int, int], encoding_dim: int, h_dims: List[int]):\n",
    "        super(LinearEncoder, self).__init__()\n",
    "        # define list of hidden layers\n",
    "        self.h_layers = nn.ModuleList()\n",
    "        # define list of hidden layer dimensions\n",
    "        self.h_dims = [input_dims[0] * input_dims[1], *h_dims, encoding_dim]\n",
    "        # loop through all hidden layers\n",
    "        for i in range(len(self.h_dims) - 1):\n",
    "            # add linear transformation to list of hidden layers\n",
    "            self.h_layers.append(nn.Linear(self.h_dims[i], self.h_dims[i + 1]))\n",
    "        # define output layer\n",
    "        self.out_layer = nn.Linear(self.h_dims[-1], encoding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten input\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # loop through all hidden layers\n",
    "        for layer in self.h_layers:\n",
    "            # apply linear transformation\n",
    "            x = layer(x)\n",
    "            # apply activation function\n",
    "            x = nn.functional.sigmoid(x)\n",
    "        # apply linear transformation to output layer\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# define decoder class which will take in a 1d tensor and transform it into a 2d tensor\n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dims: Tuple[int, int], h_dims: List[int]):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "        # define list of hidden layers\n",
    "        self.h_layers = nn.ModuleList()\n",
    "        self.output_dims = output_dims\n",
    "        # define list of hidden layer dimensions\n",
    "        self.h_dims = [input_dim, *h_dims, output_dims[0] * output_dims[1]]\n",
    "        # loop through all hidden layers\n",
    "        for i in range(len(self.h_dims) - 1):\n",
    "            # add linear transformation to list of hidden layers\n",
    "            self.h_layers.append(nn.Linear(self.h_dims[i], self.h_dims[i + 1]))\n",
    "        # define output layer\n",
    "        self.out_layer = nn.Linear(self.h_dims[-1], output_dims[0] * output_dims[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # loop through all hidden layers\n",
    "        for layer in self.h_layers:\n",
    "            # apply linear transformation\n",
    "            x = layer(x)\n",
    "            # apply activation function\n",
    "            x = nn.functional.sigmoid(x)\n",
    "        # apply linear transformation to output layer\n",
    "        x = self.out_layer(x)\n",
    "        # reshape output\n",
    "        x = x.view(x.size(0), *self.output_dims)\n",
    "        return x\n",
    "\n",
    "\n",
    "# define autoencoder class which will take in a sliding window of stock data 2d tensor\n",
    "# and output a 2d tensor of the same size\n",
    "class LinearAE(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim, h_dims, h_activ, out_activ):\n",
    "        super(LinearAE, self).__init__()\n",
    "        # define encoder\n",
    "        self.encoder = LinearEncoder(input_dim, encoding_dim, h_dims)\n",
    "        # define decoder\n",
    "        self.decoder = LinearDecoder(encoding_dim, input_dim, h_dims)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_prime = self.decoder(z)\n",
    "        return x_prime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\_tensor.py:760: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "linear_ae = LinearAE(\n",
    "  input_dim=(30, sp500_df.shape[1]),\n",
    "  encoding_dim=32,\n",
    "  h_dims=[128, 64],\n",
    "  h_activ=None,\n",
    "  out_activ=None\n",
    ")\n",
    "# move model to GPU\n",
    "linear_ae = linear_ae.to('cuda')\n",
    "# get sample data\n",
    "sample_data = sp500_df.iloc[:30].values\n",
    "sample_data = torch.tensor(sample_data, dtype=torch.float32, device='cuda').resize(1, 30, sp500_df.shape[1])\n",
    "\n",
    "# get output of model\n",
    "output = linear_ae(sample_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining Sliding Window Data Loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, df, window_size=10):\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] - self.window_size + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.df.iloc[idx:idx + self.window_size].values, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "all_data_dataset = SlidingWindowDataset(sp500_df, window_size=30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "all_data_dataloader = DataLoader(all_data_dataset, batch_size=256, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "linear_ae = LinearAE(\n",
    "  input_dim=(30, sp500_df.shape[1]),\n",
    "  encoding_dim=32,\n",
    "  h_dims=[512, 128, 64],\n",
    "  h_activ=None,\n",
    "  out_activ=None\n",
    ")\n",
    "# move model to GPU\n",
    "linear_ae = linear_ae.to('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining training loop for non LSTM autoencoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maciej\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.5894477367401123\n",
      "Epoch: 2, Loss: 1.9995976686477661\n",
      "Epoch: 3, Loss: 1.9774116277694702\n",
      "Epoch: 4, Loss: 2.0243165493011475\n",
      "Epoch: 5, Loss: 2.040438413619995\n",
      "Epoch: 6, Loss: 2.0385499000549316\n",
      "Epoch: 7, Loss: 2.035195827484131\n",
      "Epoch: 8, Loss: 2.0331673622131348\n",
      "Epoch: 9, Loss: 2.031864881515503\n",
      "Epoch: 10, Loss: 2.030794382095337\n",
      "Epoch: 11, Loss: 2.0298213958740234\n",
      "Epoch: 12, Loss: 2.02893328666687\n",
      "Epoch: 13, Loss: 2.028135061264038\n",
      "Epoch: 14, Loss: 2.0274240970611572\n",
      "Epoch: 15, Loss: 2.026794195175171\n",
      "Epoch: 16, Loss: 2.026237726211548\n",
      "Epoch: 17, Loss: 2.025747060775757\n",
      "Epoch: 18, Loss: 2.0253143310546875\n",
      "Epoch: 19, Loss: 2.0249335765838623\n",
      "Epoch: 20, Loss: 2.024599075317383\n",
      "Epoch: 21, Loss: 2.0243067741394043\n",
      "Epoch: 22, Loss: 2.0240495204925537\n",
      "Epoch: 23, Loss: 2.02382230758667\n",
      "Epoch: 24, Loss: 2.023622751235962\n",
      "Epoch: 25, Loss: 2.0234479904174805\n",
      "Epoch: 26, Loss: 2.0232958793640137\n",
      "Epoch: 27, Loss: 2.023163318634033\n",
      "Epoch: 28, Loss: 2.0230486392974854\n",
      "Epoch: 29, Loss: 2.0229499340057373\n",
      "Epoch: 30, Loss: 2.022865056991577\n",
      "Epoch: 31, Loss: 2.0227925777435303\n",
      "Epoch: 32, Loss: 2.0227315425872803\n",
      "Epoch: 33, Loss: 2.0226805210113525\n",
      "Epoch: 34, Loss: 2.0226383209228516\n",
      "Epoch: 35, Loss: 2.022603988647461\n",
      "Epoch: 36, Loss: 2.0225772857666016\n",
      "Epoch: 37, Loss: 2.0225565433502197\n",
      "Epoch: 38, Loss: 2.0225417613983154\n",
      "Epoch: 39, Loss: 2.0225322246551514\n",
      "Epoch: 40, Loss: 2.0225272178649902\n",
      "Epoch: 41, Loss: 2.022526502609253\n",
      "Epoch: 42, Loss: 2.0225296020507812\n",
      "Epoch: 43, Loss: 2.022535800933838\n",
      "Epoch: 44, Loss: 2.022545337677002\n",
      "Epoch: 45, Loss: 2.022557497024536\n",
      "Epoch: 46, Loss: 2.0225718021392822\n",
      "Epoch: 47, Loss: 2.0225884914398193\n",
      "Epoch: 48, Loss: 2.0226070880889893\n",
      "Epoch: 49, Loss: 2.022627115249634\n",
      "Epoch: 50, Loss: 2.022648811340332\n",
      "Epoch: 51, Loss: 2.022671937942505\n",
      "Epoch: 52, Loss: 2.022696018218994\n",
      "Epoch: 53, Loss: 2.022721529006958\n",
      "Epoch: 54, Loss: 2.02274751663208\n",
      "Epoch: 55, Loss: 2.0227744579315186\n",
      "Epoch: 56, Loss: 2.0228018760681152\n",
      "Epoch: 57, Loss: 2.0228302478790283\n",
      "Epoch: 58, Loss: 2.0228586196899414\n",
      "Epoch: 59, Loss: 2.022887945175171\n",
      "Epoch: 60, Loss: 2.0229172706604004\n",
      "Epoch: 61, Loss: 2.022946834564209\n",
      "Epoch: 62, Loss: 2.022976875305176\n",
      "Epoch: 63, Loss: 2.0230066776275635\n",
      "Epoch: 64, Loss: 2.0230367183685303\n",
      "Epoch: 65, Loss: 2.023066759109497\n",
      "Epoch: 66, Loss: 2.023096799850464\n",
      "Epoch: 67, Loss: 2.0231268405914307\n",
      "Epoch: 68, Loss: 2.0231568813323975\n",
      "Epoch: 69, Loss: 2.023186445236206\n",
      "Epoch: 70, Loss: 2.0232162475585938\n",
      "Epoch: 71, Loss: 2.0232455730438232\n",
      "Epoch: 72, Loss: 2.0232748985290527\n",
      "Epoch: 73, Loss: 2.023303985595703\n",
      "Epoch: 74, Loss: 2.0233328342437744\n",
      "Epoch: 75, Loss: 2.0233614444732666\n",
      "Epoch: 76, Loss: 2.0233895778656006\n",
      "Epoch: 77, Loss: 2.0234174728393555\n",
      "Epoch: 78, Loss: 2.0234451293945312\n",
      "Epoch: 79, Loss: 2.023472547531128\n",
      "Epoch: 80, Loss: 2.0234994888305664\n",
      "Epoch: 81, Loss: 2.023526191711426\n",
      "Epoch: 82, Loss: 2.023552417755127\n",
      "Epoch: 83, Loss: 2.023578405380249\n",
      "Epoch: 84, Loss: 2.023603916168213\n",
      "Epoch: 85, Loss: 2.0236289501190186\n",
      "Epoch: 86, Loss: 2.023653984069824\n",
      "Epoch: 87, Loss: 2.0236783027648926\n",
      "Epoch: 88, Loss: 2.023702383041382\n",
      "Epoch: 89, Loss: 2.023725986480713\n",
      "Epoch: 90, Loss: 2.0237491130828857\n",
      "Epoch: 91, Loss: 2.0237720012664795\n",
      "Epoch: 92, Loss: 2.023794412612915\n",
      "Epoch: 93, Loss: 2.0238165855407715\n",
      "Epoch: 94, Loss: 2.0238380432128906\n",
      "Epoch: 95, Loss: 2.0238592624664307\n",
      "Epoch: 96, Loss: 2.0238800048828125\n",
      "Epoch: 97, Loss: 2.0239005088806152\n",
      "Epoch: 98, Loss: 2.0239202976226807\n",
      "Epoch: 99, Loss: 2.023940086364746\n",
      "Epoch: 100, Loss: 2.0239593982696533\n",
      "Epoch: 101, Loss: 2.023977756500244\n",
      "Epoch: 102, Loss: 2.023996353149414\n",
      "Epoch: 103, Loss: 2.0240142345428467\n",
      "Epoch: 104, Loss: 2.0240318775177\n",
      "Epoch: 105, Loss: 2.0240492820739746\n",
      "Epoch: 106, Loss: 2.0240659713745117\n",
      "Epoch: 107, Loss: 2.0240821838378906\n",
      "Epoch: 108, Loss: 2.0240981578826904\n",
      "Epoch: 109, Loss: 2.024113893508911\n",
      "Epoch: 110, Loss: 2.0241291522979736\n",
      "Epoch: 111, Loss: 2.024143934249878\n",
      "Epoch: 112, Loss: 2.0241587162017822\n",
      "Epoch: 113, Loss: 2.02417254447937\n",
      "Epoch: 114, Loss: 2.024186372756958\n",
      "Epoch: 115, Loss: 2.0241997241973877\n",
      "Epoch: 116, Loss: 2.024212598800659\n",
      "Epoch: 117, Loss: 2.0242254734039307\n",
      "Epoch: 118, Loss: 2.024237632751465\n",
      "Epoch: 119, Loss: 2.02424955368042\n",
      "Epoch: 120, Loss: 2.024260997772217\n",
      "Epoch: 121, Loss: 2.0242722034454346\n",
      "Epoch: 122, Loss: 2.0242831707000732\n",
      "Epoch: 123, Loss: 2.0242934226989746\n",
      "Epoch: 124, Loss: 2.024303913116455\n",
      "Epoch: 125, Loss: 2.0243136882781982\n",
      "Epoch: 126, Loss: 2.024322986602783\n",
      "Epoch: 127, Loss: 2.024332284927368\n",
      "Epoch: 128, Loss: 2.024341106414795\n",
      "Epoch: 129, Loss: 2.0243496894836426\n",
      "Epoch: 130, Loss: 2.024357795715332\n",
      "Epoch: 131, Loss: 2.0243654251098633\n",
      "Epoch: 132, Loss: 2.0243732929229736\n",
      "Epoch: 133, Loss: 2.0243802070617676\n",
      "Epoch: 134, Loss: 2.0243871212005615\n",
      "Epoch: 135, Loss: 2.0243937969207764\n",
      "Epoch: 136, Loss: 2.024399995803833\n",
      "Epoch: 137, Loss: 2.0244059562683105\n",
      "Epoch: 138, Loss: 2.024411678314209\n",
      "Epoch: 139, Loss: 2.0244171619415283\n",
      "Epoch: 140, Loss: 2.0244221687316895\n",
      "Epoch: 141, Loss: 2.0244269371032715\n",
      "Epoch: 142, Loss: 2.0244317054748535\n",
      "Epoch: 143, Loss: 2.0244357585906982\n",
      "Epoch: 144, Loss: 2.024439811706543\n",
      "Epoch: 145, Loss: 2.0244433879852295\n",
      "Epoch: 146, Loss: 2.024446964263916\n",
      "Epoch: 147, Loss: 2.0244498252868652\n",
      "Epoch: 148, Loss: 2.0244529247283936\n",
      "Epoch: 149, Loss: 2.0244555473327637\n",
      "Epoch: 150, Loss: 2.0244579315185547\n",
      "Epoch: 151, Loss: 2.0244598388671875\n",
      "Epoch: 152, Loss: 2.0244617462158203\n",
      "Epoch: 153, Loss: 2.024463415145874\n",
      "Epoch: 154, Loss: 2.0244646072387695\n",
      "Epoch: 155, Loss: 2.024465799331665\n",
      "Epoch: 156, Loss: 2.0244665145874023\n",
      "Epoch: 157, Loss: 2.0244672298431396\n",
      "Epoch: 158, Loss: 2.0244674682617188\n",
      "Epoch: 159, Loss: 2.0244674682617188\n",
      "Epoch: 160, Loss: 2.0244674682617188\n",
      "Epoch: 161, Loss: 2.0244672298431396\n",
      "Epoch: 162, Loss: 2.0244665145874023\n",
      "Epoch: 163, Loss: 2.024465799331665\n",
      "Epoch: 164, Loss: 2.0244648456573486\n",
      "Epoch: 165, Loss: 2.024463415145874\n",
      "Epoch: 166, Loss: 2.0244619846343994\n",
      "Epoch: 167, Loss: 2.0244603157043457\n",
      "Epoch: 168, Loss: 2.024458408355713\n",
      "Epoch: 169, Loss: 2.024456262588501\n",
      "Epoch: 170, Loss: 2.024454116821289\n",
      "Epoch: 171, Loss: 2.024451494216919\n",
      "Epoch: 172, Loss: 2.0244486331939697\n",
      "Epoch: 173, Loss: 2.0244457721710205\n",
      "Epoch: 174, Loss: 2.0244429111480713\n",
      "Epoch: 175, Loss: 2.0244393348693848\n",
      "Epoch: 176, Loss: 2.0244357585906982\n",
      "Epoch: 177, Loss: 2.0244321823120117\n",
      "Epoch: 178, Loss: 2.024428129196167\n",
      "Epoch: 179, Loss: 2.0244240760803223\n",
      "Epoch: 180, Loss: 2.0244197845458984\n",
      "Epoch: 181, Loss: 2.0244152545928955\n",
      "Epoch: 182, Loss: 2.0244107246398926\n",
      "Epoch: 183, Loss: 2.0244057178497314\n",
      "Epoch: 184, Loss: 2.0244007110595703\n",
      "Epoch: 185, Loss: 2.02439546585083\n",
      "Epoch: 186, Loss: 2.02439022064209\n",
      "Epoch: 187, Loss: 2.0243844985961914\n",
      "Epoch: 188, Loss: 2.024378776550293\n",
      "Epoch: 189, Loss: 2.0243728160858154\n",
      "Epoch: 190, Loss: 2.024367094039917\n",
      "Epoch: 191, Loss: 2.0243608951568604\n",
      "Epoch: 192, Loss: 2.0243542194366455\n",
      "Epoch: 193, Loss: 2.0243477821350098\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.21 GiB already allocated; 0 bytes free; 7.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [61], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     11\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 12\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[1;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 23\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     25\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\optim\\adam.py:234\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure, grad_scaler)\u001B[0m\n\u001B[0;32m    231\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    232\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 234\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[43m         \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[43m         \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    240\u001B[0m \u001B[43m         \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m         \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m         \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m         \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m         \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m         \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[43m         \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[43m         \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m         \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m         \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\optim\\adam.py:300\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    298\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 300\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    303\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    305\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    306\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    307\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    308\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    309\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\optim\\adam.py:410\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    408\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 410\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m    412\u001B[0m param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.21 GiB already allocated; 0 bytes free; 7.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(linear_ae.parameters(), lr=lr)\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in all_data_dataloader:\n",
    "        batch = batch.to('cuda')\n",
    "        output = linear_ae(batch)\n",
    "        loss = nn.functional.mse_loss(output, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
